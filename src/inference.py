import sys
import os

# --- 1. è·¯å¾„è®¾ç½® (ç¡®ä¿èƒ½å¯¼å…¥æ ¹ç›®å½•çš„ config.py) ---
current_dir = os.path.dirname(os.path.abspath(__file__))
root_dir = os.path.dirname(current_dir)
sys.path.append(root_dir)

import config  # å¯¼å…¥é…ç½®æ–‡ä»¶
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# --- 2. åŠ è½½æ¨¡å‹ä¸åˆ†è¯å™¨ ---
MERGED_MODEL_PATH = config.LLM_MODEL_PATH

print(f"ğŸ”„ [Inference] æ­£åœ¨åŠ è½½å¤§æ¨¡å‹: {MERGED_MODEL_PATH} ...")

if not os.path.exists(MERGED_MODEL_PATH):
    # å¦‚æœæ‰¾ä¸åˆ°ï¼Œå°è¯•ä½¿ç”¨ config é‡Œå®šä¹‰çš„åŸå§‹è·¯å¾„ï¼Œæˆ–è€…æ˜¯ HuggingFace ID
    print(f"âš ï¸ è­¦å‘Š: è·¯å¾„ {MERGED_MODEL_PATH} ä¸å­˜åœ¨ã€‚")
    print("è¯·æ£€æŸ¥ config.py é…ç½®ï¼Œæˆ–ç¡®è®¤æ¨¡å‹æ–‡ä»¶å·²æ”¾å…¥æŒ‡å®šæ–‡ä»¶å¤¹ã€‚")

try:
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        MERGED_MODEL_PATH,
        trust_remote_code=True
    )

    # åŠ è½½æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        MERGED_MODEL_PATH,
        torch_dtype=torch.bfloat16,  # æ˜¾å­˜ä¼˜åŒ–
        device_map=config.DEVICE,    # ä½¿ç”¨ config ä¸­çš„è®¾å¤‡ (cuda/cpu)
        trust_remote_code=True
    )
    model.eval() # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
    print("âœ… [Inference] å¤§æ¨¡å‹åŠ è½½å®Œæˆï¼")

except Exception as e:
    print(f"âŒ [Inference] æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
    # è¿™é‡Œä¸æŠ›å‡ºå¼‚å¸¸ï¼Œé˜²æ­¢æ•´ä¸ª app å´©æºƒï¼Œä½†åœ¨å®é™…è°ƒç”¨æ—¶ä¼šæŠ¥é”™
    model = None
    tokenizer = None


# --- 3. æ ¸å¿ƒåŠŸèƒ½å‡½æ•° ---

def _build_prompt_messages(user_question: str, rag_context: str) -> list:
    """æ„å»º RAG æ¨¡å¼çš„æç¤ºè¯ (System Prompt)"""
    system_content = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šåŒ»å­¦åŠ©æ‰‹ã€‚ä½ çš„ä»»åŠ¡æ˜¯åŸºäºæä¾›çš„ä¿¡æ¯ï¼Œå¯¹ç”¨æˆ·çš„é—®é¢˜ç»™å‡ºå‡†ç¡®ã€ç›¸å…³ä¸”ç®€æ´çš„å›ç­”ï¼Œä½ çš„è¯­æ°”åº”è®©ä½ æ˜¾å¾—ä¸“ä¸šä¸”æœ‰åŒç†å¿ƒã€‚è¯·éµå¾ªä»¥ä¸‹æŒ‡å—ï¼š
1. è¯·ä½¿ç”¨ã€å‚è€ƒææ–™ã€‘ä¸­çš„ä¿¡æ¯æ¥å›ç­”é—®é¢˜ï¼Œå¦‚æœã€å‚è€ƒææ–™ã€‘ä¸­çš„ä¿¡æ¯èƒ½ç›´æ¥å›ç­”é—®é¢˜ï¼Œåˆ™å°½é‡ä½¿ç”¨åŸæ–‡è¯­æ®µï¼Œå¹¶ä¸”ä¹‹ååº”å½“è¡¥å……ä¸Šä½ æ‰€çŸ¥çš„å…¶ä»–ä¿¡æ¯ã€‚
2. å¦‚æœã€å‚è€ƒææ–™ã€‘ä¸­çš„ä¿¡æ¯ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œåˆ™ä½ åº”è¯¥å›ç­”â€œæˆ‘ä¸çŸ¥é“â€ã€‚
3. åœ¨ä½ ç”Ÿæˆå›ç­”æ—¶ï¼Œåº”ä¿æŒå›ç­”çš„è¿è´¯æ€§å’Œé€»è¾‘æ€§ã€‚
4. å›ç­”ä¸­ç»å¯¹ä¸èƒ½åŒ…æ‹¬â€œè¯·ç»™å‡ºæ­£ç¡®ç­”æ¡ˆå¹¶è¯´æ˜ç†ç”±â€è¿™æ®µè¯ã€‚
---
ã€å‚è€ƒèµ„æ–™ã€‘
{rag_context}
---
"""
    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_question}
    ]
    return messages


def _build_normal_messages(user_question: str, rag_context: str) -> list:
    """æ„å»ºæ™®é€šæ¨¡å¼çš„æç¤ºè¯"""
    system_content = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„åŒ»å­¦åŠ©æ‰‹ã€‚è¯·å›ç­”é—®é¢˜ã€‚"""
    messages = [
        {"role": "system", "content": system_content},
        {"role": "user", "content": user_question}
    ]
    return messages


def get_medical_answer(user_question: str, rag_context: str) -> str:
    """è·å–åŒ»å­¦å›ç­” (RAGå¢å¼ºç‰ˆ)"""
    if model is None: return "âŒ æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼Œæ— æ³•å›ç­”ã€‚"
    
    try:
        messages = _build_prompt_messages(user_question, rag_context)
        
        inputs = tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
            add_generation_prompt=True
        ).to(model.device) 
        
        generate_ids = model.generate(
            inputs,
            max_new_tokens=1024,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )
        
        input_length = inputs.shape[1]
        response = tokenizer.decode(
            generate_ids[0][input_length:], 
            skip_special_tokens=True
        )
        return response

    except Exception as e:
        print(f"æ¨ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "æŠ±æ­‰ï¼Œæ¨¡å‹åœ¨å›ç­”æ—¶é‡åˆ°äº†ä¸€ä¸ªå†…éƒ¨é”™è¯¯ã€‚"


def get_normal_answer(user_question: str, rag_context: str) -> str:
    """è·å–æ™®é€šå›ç­” (è£¸å¥”ç‰ˆ)"""
    if model is None: return "âŒ æ¨¡å‹æœªæˆåŠŸåŠ è½½ï¼Œæ— æ³•å›ç­”ã€‚"

    try:
        # æ™®é€šå›ç­”ä¸éœ€è¦ rag_contextï¼Œä¼ ç©ºå­—ç¬¦ä¸²æˆ–å¿½ç•¥å³å¯
        messages = _build_normal_messages(user_question, "")

        inputs = tokenizer.apply_chat_template(
            messages,
            return_tensors="pt",
            add_generation_prompt=True
        ).to(model.device)

        generate_ids = model.generate(
            inputs,
            max_new_tokens=1024,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.pad_token_id,
            do_sample=True,
            temperature=0.7,
            top_p=0.9,
        )

        input_length = inputs.shape[1]
        response = tokenizer.decode(
            generate_ids[0][input_length:],
            skip_special_tokens=True
        )
        return response

    except Exception as e:
        print(f"æ¨ç†æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return "æŠ±æ­‰ï¼Œæ¨¡å‹åœ¨å›ç­”æ—¶é‡åˆ°äº†ä¸€ä¸ªå†…éƒ¨é”™è¯¯ã€‚"


# --- 4. è‡ªæˆ‘æµ‹è¯• (å½“ç›´æ¥è¿è¡Œæ­¤æ–‡ä»¶æ—¶) ---
if __name__ == "__main__":
    print("\n--- æ­£åœ¨æ‰§è¡Œè‡ªæˆ‘æµ‹è¯•... ---")
    
    dummy_context = """
    é˜¿å¸åŒ¹æ—ï¼ˆAspirinï¼‰æ˜¯ä¸€ç§æ°´æ¨é…¸ç›è¯ç‰©ï¼Œå¸¸ç”¨äºæ²»ç–—ç–¼ç—›ã€å‘çƒ­å’Œç‚ç—‡ã€‚
    å®ƒè¿˜å¯ä»¥é€šè¿‡æŠ‘åˆ¶è¡€å°æ¿èšé›†æ¥é¢„é˜²å¿ƒè„ç—…å‘ä½œå’Œä¸­é£ã€‚
    å¸¸è§å‰¯ä½œç”¨åŒ…æ‹¬èƒƒè‚ é“ä¸é€‚ã€æ¶å¿ƒå’Œå‡ºè¡€é£é™©å¢åŠ ã€‚
    """
    dummy_question = "é˜¿å¸åŒ¹æ—æœ‰ä»€ä¹ˆç”¨ï¼Ÿå®ƒæœ‰ä»€ä¹ˆå‰¯ä½œç”¨ï¼Ÿ"
    
    print(f"æµ‹è¯•é—®é¢˜: {dummy_question}")
    
    if model:
        answer = get_medical_answer(dummy_question, dummy_context)
        print("\n--- æ¨¡å‹çš„å›ç­” ---")
        print(answer)
        print("------------------")
    else:
        print("âŒ æ¨¡å‹æœªåŠ è½½ï¼Œè·³è¿‡æ¨ç†æµ‹è¯•ã€‚")
    
    print("\n--- è‡ªæˆ‘æµ‹è¯•å®Œæˆ ---")